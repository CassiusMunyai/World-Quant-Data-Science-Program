# Applied Data Science Projects

This repository contains the outcomes of eight end-to-end applied data science projects completed as part of a structured learning program. Each project demonstrates the full data science workflow, from acquiring and cleaning raw data to building machine learning models and communicating insights.

Program Structure

Each project consists of four self-paced lessons followed by a capstone assignment.

Assignments are programmatically graded, ensuring reproducibility and rigor.

To pass, learners must achieve a minimum score of 90% on each assessment.

The projects cover a range of real-world datasets, problem types, and technical tools, building both breadth and depth in data science skills.


Projects Overview
1. Housing in Mexico
Objective: Explore whether real estate prices depend more on property size or location.
Skills: Data import and cleaning (CSV), exploratory data analysis, correlation analysis, data visualization.
Dataset: 21,000 properties across Mexico.

2. Apartment Sales in Buenos Aires
Objective: Predict apartment prices in Argentina using regression.
Skills: Linear regression, pipeline construction (imputation, encoding), reducing overfitting.
Dataset: Apartment sales records.

3. Air Quality in Nairobi
Objective: Forecast particulate matter (PM) levels.
Skills: Time-series analysis (ARMA model), data extraction with MongoDB & PyMongo, hyperparameter tuning.
Dataset: Air quality measurements from Nairobi.

4. Earthquake Damage in Nepal
Objective: Predict building damage after earthquakes.
Skills: Logistic regression, decision trees, bias detection in datasets, data extraction with SQLite.
Dataset: Building attributes and earthquake outcomes.

5. Bankruptcy in Poland
Objective: Predict whether companies will go bankrupt.
Skills: Random forests, gradient boosting, class imbalance handling (resampling), Linux command line navigation, evaluation with precision/recall trade-offs.
Dataset: Financial records of Polish companies.

6. Customer Segmentation in the US
Objective: Segment U.S. consumers into meaningful groups.
Skills: K-means clustering, PCA for dimensionality reduction and visualization, interactive dashboard creation with Plotly Dash.
Dataset: Consumer demographic and behavioral data.

7. A/B Testing at WorldQuant University
Objective: Test whether sending an email increases program enrollment.
Skills: Hypothesis testing (chi-square), custom ETL pipeline design with Python classes, interactive application development with a three-tiered design pattern.
Dataset: Experimental enrollment data.

8. Volatility Forecasting in India
Objective: Forecast financial asset volatility.
Skills: Time-series modeling (GARCH), API-based data acquisition, SQLite for storage, building and serving a custom API.
Dataset: Stock market data from India.

Key Takeaways

Across all projects, learners:
Worked with multiple data sources: CSVs, APIs, SQL/NoSQL databases (SQLite, MongoDB).
Implemented end-to-end ETL pipelines for reliable, repeatable workflows.
Built models spanning supervised, unsupervised, and time-series learning.
Applied data visualization and dashboards to communicate insights.
Practiced evaluating trade-offs between metrics (e.g., precision vs. recall).
Considered ethical implications such as bias and fairness in data.

Tools & Technologies
Languages: Python
Libraries: Pandas, NumPy, scikit-learn, statsmodels, matplotlib, seaborn, Plotly Dash
Databases: SQLite, MongoDB
Other Skills: Linux CLI, API design & consumption, ETL automation, model evaluation & communication

This collection demonstrates readiness to handle real-world data problems end-to-end, with both technical depth and applied context.
